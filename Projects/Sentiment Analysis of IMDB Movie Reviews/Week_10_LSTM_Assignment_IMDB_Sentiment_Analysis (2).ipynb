{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_10_LSTM_Assignment_IMDB_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMucfLUS1yhH"
      },
      "source": [
        "## What is this?\n",
        "\n",
        "This Jupyter Notebook contains Python code for building a LSTM Recurrent Neural Network that gives 87-88% accuracy on the IMDB Movie Review Sentiment Analysis Dataset. \n",
        "\n",
        "More information is given on [this blogpost](https://www.bouvet.no/bouvet-deler/explaining-recurrent-neural-networks).\n",
        "\n",
        "This code is partly based on [markwest1972's](https://github.com/markwest1972/LSTM-Example-Google-Colaboratory) Notebook on LSTM-Example-Google-Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFUKGe4x3ala"
      },
      "source": [
        "## Built for Google Collaboratory\n",
        "\n",
        "Train your network more quickly in Google Collaboratory. From the **Runtime** menu select **Change Runtime** Type and choose \"GPU\"!\n",
        "\n",
        "Don't forget to select **Runtime** -> **Restart runtime** to put your changes into effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP1VrbVp3sVu"
      },
      "source": [
        "## Setting up\n",
        "\n",
        "When running this for the first time you may get a warning telling you to restart the Runtime. You can ignore this, but feel free to select \"Runtime->Restart Runtime\" from the overhead menu if you encounter problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e3txwbh3q76"
      },
      "source": [
        "# Imports\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "\n",
        "# Supress deprecation warnings\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "# Fetch \"IMDB Movie Review\" data from tensorflow (https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb)\n",
        "# and constrain the reviews to 10000 most commonly used words.\n",
        "# Note here the vocab_size is equal to 10000 common words.\n",
        "### START CODE HERE ###\n",
        "vocab_size = None\n",
        "(x_train, y_train), (x_test, y_test) = None\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Map for readable classnames\n",
        "class_names = [\"Negative\", \"Positive\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdyHL8FF0JJy"
      },
      "source": [
        "## Create map for converting IMDB dataset to readable reviews\n",
        "\n",
        "Reviews in the IMDB dataset have been encoded as a sequence of integers. Luckily the dataset also \n",
        "contains an index for converting the reviews back into human readable form.\n",
        "This is the process of creating a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E05AweFu0Imt"
      },
      "source": [
        "# Get the word index from the dataset\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "\n",
        "# Ensure that \"special\" words are mapped into human readable terms \n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNKNOWN>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "# Perform reverse word lookup and make it callable\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFXK-g6G81sC"
      },
      "source": [
        "## Data Insight\n",
        "\n",
        "Here we take a closer look at our data. How many words do our reviews contain?\n",
        "\n",
        "And what do our review look like in machine and human readable form?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD1qHVBn81Y_"
      },
      "source": [
        "# Concatenate x_train and x_test datasets and return it as allreviews.\n",
        "### START CODE HERE ###\n",
        "allreviews = None\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"Maximum review length: {}\".format(len(max((allreviews), key=len))))\n",
        "print(\"Minimum review length: {}\".format(len(min((allreviews), key=len))))\n",
        "\n",
        "# Use allreviews to calculate the lengths across the training and test dataset and return as result.\n",
        "### START CODE HERE ###\n",
        "result = None\n",
        "### END CODE HERE ###\n",
        "print(\"Mean review length: {}\".format(np.mean(result)))\n",
        "\n",
        "\n",
        "# Print a review and it's class as stored in the dataset. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Machine readable Review\")\n",
        "print(\"  Review Text: \" + str(x_train[60]))\n",
        "print(\"  Review Sentiment: \" + str(y_train[60]))\n",
        "\n",
        "# Print a review and it's class in human readable format. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Human Readable Review\")\n",
        "print(\"  Review Text: \" + decode_review(x_train[60]))\n",
        "print(\"  Review Sentiment: \" + class_names[y_train[60]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF-Votm66zD5"
      },
      "source": [
        "## Pre-processing Data\n",
        "\n",
        "We need to make sure that our reviews are of a uniform length. This is for the LSTM's parameters.\n",
        "\n",
        "Some reviews will need to be truncated, while others need to be [padded](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences).\n",
        "Further reading on [Padding for NLP](https://medium.com/@canerkilinc/padding-for-nlp-7dd8598c916a)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNtJTLJA6gaT"
      },
      "source": [
        "# The length of reviews\n",
        "review_length = 500\n",
        "\n",
        "# Padding / truncated our reviews\n",
        "x_train = sequence.pad_sequences(x_train, maxlen = review_length)\n",
        "# Perform the same for x_test\n",
        "### START CODE HERE ###\n",
        "x_test = None\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Check the size of our datasets. Review data for both test and training should \n",
        "# contain 25000 reviews of 500 integers. Class data should contain 25000 values, \n",
        "# one for each review. Class values are 0 or 1, indicating a negative \n",
        "# or positive review.\n",
        "print(\"Shape Training Review Data: \" + str(x_train.shape))\n",
        "print(\"Shape Training Class Data: \" + str(y_train.shape))\n",
        "print(\"Shape Test Review Data: \" + str(x_test.shape))\n",
        "print(\"Shape Test Class Data: \" + str(y_test.shape))\n",
        "\n",
        "# Note padding is added to start of review, not the end\n",
        "print(\"\")\n",
        "print(\"Human Readable Review Text (post padding): \" + decode_review(x_train[60]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfOdV_VCCFee"
      },
      "source": [
        "## Create and build LSTM Recurrent Neural Network\r\n",
        "\r\n",
        "You will be using a Sequential Model to build the LSTM. A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. The other way to build a model is by using the Functional API. Take a look at [Sequential API vs Functional API model in Keras](https://medium.com/@hanify/sequential-api-vs-functional-api-model-in-keras-266823d7cd5e) to read about the differences between the two and how to create a Sequential and a Functional API.\r\n",
        "\r\n",
        "References: \r\n",
        "- [Keras Sequential API](https://keras.io/guides/sequential_model/)\r\n",
        "- [Embedding Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\r\n",
        "- [Dropout Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)\r\n",
        "- [CuDNNLSTM](https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/layers/CuDNNLSTM)\r\n",
        "- [Dense Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\r\n",
        "- [Binary Crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\r\n",
        "- [Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nmO8M4aCKwT"
      },
      "source": [
        "# We begin by defining a empty stack called model using tf.keras.Sequential.\n",
        "# We'll use this for building our network, layer by layer.\n",
        "### START CODE HERE ###\n",
        "model = None\n",
        "### END CODE HERE ###\n",
        "\n",
        "# The Embedding Layer provides a spatial mapping (or Word Embedding) of all the \n",
        "# individual words in our training set. Words close to one another share context \n",
        "# and or meaning. This spatial mapping is learned during the training process.\n",
        "\n",
        "# To create a Embedding Layer, use the following parameters:\n",
        "#   - input_dim as your vocab_size\n",
        "#   - output_dim as 32 to which each words shall be mapped\n",
        "#   - the input_length will be review_length, the length of input sequences\n",
        "### START CODE HERE ###\n",
        "model.add(None)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Dropout layers fight overfitting and forces the model to learn multiple \n",
        "# representations of the same data by randomly disabling neurons in the \n",
        "# learning phase.\n",
        "\n",
        "# To create a Dropout layer, use the rate as 0.25. This will disable 25% of neurons\n",
        "### START CODE HERE ###\n",
        "model.add(None)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# We are using a fast version of LSTM whih is optimised for GPUs. This layer \n",
        "# looks at the sequence of words in the review, along with their word embeddings\n",
        "# and uses both of these to determine to sentiment of a given review.\n",
        "\n",
        "# To create the fast LSTM, use CuDNNLSTM with 32 units.\n",
        "# This will be the LSTM units in this layer.\n",
        "### START CODE HERE ###\n",
        "model.add(None)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Add a second dropout layer with the same aim as the first with a 25% dropout rate.\n",
        "### START CODE HERE ###\n",
        "model.add(None)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# All LSTM units are connected to a single node in the dense layer. A sigmoid \n",
        "# activation function determines the output from this node - a value \n",
        "# between 0 and 1. Closer to 0 indicates a negative review. Closer to 1 \n",
        "# indicates a positive review.\n",
        "\n",
        "# To create a Dense Layer use a single unit and use sigmoid as the activation function\n",
        "### START CODE HERE ###\n",
        "model.add(None)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Compile the model\n",
        "# To compile the model use the following parameters:\n",
        "#   - Keras Binary Cross Entropy as the loss funtion\n",
        "#   - Adam as the Optimizer\n",
        "#   - Accuracy as the reporting metrics\n",
        "### START CODE HERE ###\n",
        "model.compile(None)\n",
        "### END CODE HERE ###\n",
        "\n",
        "\n",
        "# Display a summary of the models structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xx1Q2I8WNI9"
      },
      "source": [
        "## Visualise the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz0Erj2WU3Vh"
      },
      "source": [
        "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KdfAoHsGwzo"
      },
      "source": [
        "## Train the LSTM\r\n",
        "\r\n",
        "Use the [fit method](https://keras.io/api/models/model_training_apis/#fit-method) to Train the LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEN1vV4nG1V3"
      },
      "source": [
        "# Train the LSTM for 3 epochs by feeding in the training data and set validation split as 0.2 and return the logs \n",
        "# and save it as history.\n",
        "# Use 256 as batch size.\n",
        "### START CODE HERE ###\n",
        "history = None\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpCS2-jFH1KY"
      },
      "source": [
        "## Evaluate model with test data and view results\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPnfxwbnITqV"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Use predict_classes from the Sequential API to get prediction classes for test data and create a classification report.\n",
        "### START CODE HERE ###\n",
        "predicted_classes = None\n",
        "print(classification_report(None))\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTZjzhfZKgQ5"
      },
      "source": [
        "## Create an Area under the ROC Curve Plot\r\n",
        "\r\n",
        "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. \r\n",
        "\r\n",
        "Further Reading: [Understanding AUC - ROC Curve](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qQhOyJtYPQw"
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Use predict to make predictions on the test set and return it as y_pred.\r\n",
        "# Then use sklearn's roc_curve API to calculate the false positive and true positive rate and return the threshold.\r\n",
        "# Finally, use the false positive and true positive rate to calculate the Area Under the Curve using the auc API.\r\n",
        "### START CODE HERE ###\r\n",
        "y_pred = None\r\n",
        "fpr, tpr, thresholds = None\r\n",
        "roc_auc = None\r\n",
        "### END CODE HERE ###\r\n",
        "\r\n",
        "def plot_roc_curve(fpr,tpr, roc_auc): \r\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:0.2f})')\r\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\r\n",
        "    plt.axis([0,1,0,1]) \r\n",
        "    plt.xlabel('False Positive Rate') \r\n",
        "    plt.ylabel('True Positive Rate')\r\n",
        "    plt.legend(loc=\"lower right\") \r\n",
        "    plt.show()    \r\n",
        "  \r\n",
        "plot_roc_curve (fpr,tpr, roc_auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlAfxIoTrtYa"
      },
      "source": [
        "## Run your own text against the trained model\n",
        "\n",
        "This is a fun way to test out the limits of the trained model. To avoid getting errors - type in lower case only and do not use punctuation! \n",
        "\n",
        "You'll see the raw prediction from the model - basically a value between 0 and 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEKEB0DpD_8P"
      },
      "source": [
        "# Write your own review for example:\n",
        "# review = \"this was a terrible film with too much sex and violence i walked out halfway through\"\n",
        "review = None\n",
        "\n",
        "# Encode review (replace word with integers)\n",
        "review_encoded = [word_index[word] for word in review.split(\" \")]\n",
        "\n",
        "# Ensure review_encoded is 500 words long by padding it using pad_sequences\n",
        "### START CODE HERE ###\n",
        "review_padded = None\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Run your review_padded against the trained model\n",
        "### START CODE HERE ###\n",
        "raw_prediction = model.predict(array([None][0]))[0][0]\n",
        "### END CODE HERE ###\n",
        "prediction = int(round(raw_prediction))\n",
        "\n",
        "\n",
        "# Test the model and print the result\n",
        "print(\"Review: \" + review)\n",
        "print(\"Raw Prediction: \" + str(raw_prediction))\n",
        "print(\"Predicted Class: \" + class_names[prediction])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}